Distributed AI Model App ğŸš€
This project utilizes Mistral 7B with Ray Distributed to efficiently process AI tasks across multiple devices. It enables distributed AI inference, making large language models accessible with shared compute power.

ğŸ“Œ Features
âœ… Distributed AI processing using Ray
âœ… Model inference with Mistral 7B
âœ… Scalable architecture with worker nodes
âœ… REST API & WebSocket support for real-time communication

ğŸ› ï¸ Setup Instructions
1ï¸âƒ£ Install Dependencies
Ensure Python 3.8+ is installed, then run:

bash
Copy
Edit
pip install -r requirements.txt
2ï¸âƒ£ Start the Main Server
bash
Copy
Edit
./run.sh
This will:
âœ… Start the Ray Cluster
âœ… Launch the worker nodes
âœ… Start the FastAPI server

3ï¸âƒ£ Run a Test Query
To check if the server is working, send a test request:

bash
Copy
Edit
curl -X POST "http://localhost:8000/query/" -d '{"prompt": "What is AI?"}'
If successful, youâ€™ll receive an AI-generated response.

ğŸ’» Setting Up a Worker Node
On the worker device, clone the repository and run:

bash
Copy
Edit
git clone https://github.com/Carl6105/distributed-ai-app.git
cd distributed-ai-app
chmod +x setup_worker.sh
./setup_worker.sh
This will install dependencies and connect the worker to the main Ray Cluster.

ğŸ› ï¸ Available Endpoints
1ï¸âƒ£ WebSocket API (ws://localhost:8000/ws)
For real-time streaming responses.

2ï¸âƒ£ REST API (POST /query/)
For sending requests and receiving responses.

ğŸ”¹ Author: Carl
ğŸ”¹ License: MIT
ğŸ”¹ Contributions: Open to pull requests!